{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is part of \"SymbXRL: Symbolic Explainable Deep Reinforcement Learning for Mobile Networks\" \n",
    "\n",
    "Copyright - RESILIENT AI NETWORK LAB, IMDEA NETWORKS\n",
    "\n",
    "DISCLAIMER: THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import h5py\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SAC Imports\n",
    "from constants import PROJ_ADDR\n",
    "from SAC.sac import SAC\n",
    "from SAC.replay_memory import ReplayMemory\n",
    "from SAC.smartfunc import sel_ue\n",
    "import torch\n",
    "from custom_mimo_env import MimoEnv, reverse_sel_ue\n",
    "\n",
    "\n",
    "# Action Steering Imports\n",
    "from Action_Steering.action_steering_utils import process_buffer, transform_action, do_action_steering_this_timestep, extract_decision_from_suggested \n",
    "from Action_Steering.symbolic_representation import QuantileManager, Symbolizer\n",
    "from Action_Steering.experiment_constants import KPI_LIST, USERS\n",
    "from Action_Steering.decision_graph import DecisionGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PyTorch version\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Check CUDA version\n",
    "cuda_version = torch.version.cuda\n",
    "print(\"CUDA version:\", cuda_version)\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"Is CUDA available:\", cuda_available)\n",
    "\n",
    "# If CUDA is available, print the device name\n",
    "if cuda_available:\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENT SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dataset\n",
    "H_file = h5py.File(f'{PROJ_ADDR}/A2-MIMOResourceScheduler/Datasets/LOS_highspeed2_64_7.hdf5','r')\n",
    "H = np.array(H_file.get('H'))\n",
    "print(\"H shape is:\", H.shape)\n",
    "se_max_ur = H_file.get('se_max')\n",
    "se_max_ur = np.array(se_max_ur)\n",
    "print(\"se_max_ur shape is:\", se_max_ur.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MimoEnv(H, se_max_ur)\n",
    "\n",
    "class SACArgs:\n",
    "    def __init__(self):\n",
    "        # Default values for arguments\n",
    "        self.policy = \"Gaussian\"  # Policy name (Gaussian, Deterministic)\n",
    "        self.eval = False # False : Train the agent, True : Evaluate the agent\n",
    "        self.gamma = 0.99 # Discount factor\n",
    "        self.tau = 0.005 # Target smoothing coefficient (Critic to Critic target network)\n",
    "        self.lr = 0.0003 # Learning rate for the critic network\n",
    "        self.alpha_lr = 0.0003 # Learning rate for the actor network\n",
    "        self.alpha = 0.95 # Entropy coefficient (0.0 = no entropy, 1.0 = maximum entropy)\n",
    "        self.automatic_entropy_tuning = True\n",
    "        self.seed = 1 # Random seed\n",
    "        self.batch_size = 256 # Batch size for Replay Memory\n",
    "        self.max_episode_steps = len(H) # Maximum number of steps for each episode\n",
    "        self.max_episode = 85 # Maximum number of episodes\n",
    "        self.hidden_size = 512 # Hidden size for the networks\n",
    "        self.updates_per_step = 1 # Number of updates per step\n",
    "        # self.save_per_epochs = 15 \n",
    "        self.start_steps = 3000 # Number of steps for uniform-random action selection, before running real policy. Helps exploration \n",
    "        self.target_update_interval = 1 # Value update interval for the Critic target networks \n",
    "        self.replay_size = 1000000 # Size of the replay buffer\n",
    "        self.cuda = 1 # Cuda ID to use\n",
    "        self.gpu_nums = 1 # Number of GPUs to use\n",
    "\n",
    "# Creating an instance of SACArgs with default values\n",
    "args = SACArgs()\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = len([env.action_space.sample()])\n",
    "max_actions = env.action_space.n\n",
    "\n",
    "agent = SAC(num_states, num_actions, max_actions, args, args.lr, args.alpha_lr)\n",
    "print('SAC build finished')\n",
    "ckpt_path = f\"{PROJ_ADDR}/A2-MIMOResourceScheduler/models/SACG_884.53_551_dtLOS_HS2_checkpointed.pth_\"\n",
    "agent.load_checkpoint(ckpt_path)\n",
    "memory = ReplayMemory(args.replay_size, args.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTATION WITH AS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_addpated_timestep = 0\n",
    "\n",
    "step_rewards = []\n",
    "acn_str = []\n",
    "grp_str = []\n",
    "mean_rew = []\n",
    "fin_act = []\n",
    "score = 0\n",
    "state_data = []\n",
    "action_reward_data = []\n",
    "test_buffer = []\n",
    "\n",
    "# Create the object of symbolizer and qm\n",
    "kpis = KPI_LIST\n",
    "\n",
    "# Action Steering\n",
    "do_action_steering = True\n",
    "start_action_steering_from = 5\n",
    "\n",
    "# Symbolic tools Instantiation\n",
    "qunatile_manager = QuantileManager(kpis + ['scheduled_user'])\n",
    "qunatile_manager.reset()\n",
    "qunatile_manager.partial_fit(\"scheduled_user\", [0])\n",
    "qunatile_manager.partial_fit(\"scheduled_user\", [7])\n",
    "\n",
    "# Define Symbolic df\n",
    "symbolic_df = pd.DataFrame()\n",
    "\n",
    "# Symbolizer\n",
    "symbolizer = Symbolizer(quantile_manager=qunatile_manager, kpi_list=kpis, users=USERS)\n",
    "\n",
    "rt_decision_graph = {\n",
    "    0: DecisionGraph(column_name=\"decision\"),\n",
    "    1: DecisionGraph(column_name=\"decision\"),\n",
    "    2: DecisionGraph(column_name=\"decision\"),\n",
    "}\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "grp_str.append(observation[0, 14:])\n",
    "\n",
    "action_steering_info_df = []\n",
    "\n",
    "while not done:\n",
    "    action, final_action = agent.select_action(observation)\n",
    "    fin_act.append(final_action[0])\n",
    "    ue_select, idx = sel_ue(final_action[0])\n",
    "    acn_str.append(ue_select)\n",
    "    final_reward = 0\n",
    "\n",
    "    # Code goes\n",
    "    buff_ac = []\n",
    "    buff_ac.append((observation, action))\n",
    "\n",
    "    curr_states_df, curr_actions_rewards_df = process_buffer(buff_ac, transform_action, sel_ue, mode=None, timestep=info['current_step'])\n",
    "\n",
    "    # Symbolic Representation Section\n",
    "    state_t_df = curr_states_df[curr_states_df['timestep'] == info['current_step']]\n",
    "    decision_t_df = curr_actions_rewards_df[curr_actions_rewards_df['timestep'] == info['current_step']]\n",
    "\n",
    "    symbolic_form = symbolizer.create_symbolic_form(state_t_df, decision_t_df)\n",
    "\n",
    "    if not symbolic_form.empty:\n",
    "        # Add updated timestep to the symbolic_form\n",
    "        buffer_addpated_timestep += 1\n",
    "        symbolic_form['timestep'] = [buffer_addpated_timestep] * symbolic_form.shape[0]\n",
    "        symbolic_form['reward'] = [0] * symbolic_form.shape[0]\n",
    "\n",
    "        # Create and fetch info for the action steering\n",
    "        agent_proposed_reward = env.get_reward(final_action[0])\n",
    "        # Get the reward for agent's action\n",
    "        action_steering_info = {\n",
    "            \"timestep\": buffer_addpated_timestep,\n",
    "            \"agent_decsion\": ue_select,\n",
    "            \"agent_reward\": agent_proposed_reward,\n",
    "            \"used_action_steering\": False,\n",
    "            \"action_steering_decision\": ue_select,\n",
    "            \"action_steering_reward\": agent_proposed_reward\n",
    "        }\n",
    "\n",
    "    action_steered = False\n",
    "\n",
    "    if (info['current_step'] > start_action_steering_from) and (do_action_steering):\n",
    "        suggested_decision, action_steered_reward = do_action_steering_this_timestep(symbolic_form, symbolic_df, rt_decision_graph)\n",
    "\n",
    "        if type(suggested_decision) != bool:\n",
    "            if action_steered_reward > agent_proposed_reward:\n",
    "                extracted_decisoin = extract_decision_from_suggested(suggested_decision)\n",
    "                action_steered_final = reverse_sel_ue(extracted_decisoin)\n",
    "                ue_select = extracted_decisoin\n",
    "\n",
    "                # Update the symbolizer with action_steering data\n",
    "                decision_t_df.at[0, 'action'] = extracted_decisoin\n",
    "\n",
    "                symbolic_form_as = symbolizer.create_symbolic_form(state_t_df, decision_t_df)\n",
    "\n",
    "                next_obs, reward, done, _, info = env.step(action_steered_final)\n",
    "                action_steering_info['action_steering_decision'] = extracted_decisoin\n",
    "                action_steering_info['action_steering_reward'] = reward\n",
    "                action_steering_info['used_action_steering'] = True\n",
    "                symbolic_form['decision'] = symbolic_form_as['decision']\n",
    "                symbolic_form['sched_members'] = symbolic_form_as['sched_members']\n",
    "\n",
    "                final_reward = reward\n",
    "                action_steered = True\n",
    "\n",
    "    if not action_steered:\n",
    "        next_obs, reward, done, _, info = env.step(final_action[0])\n",
    "        final_reward = reward\n",
    "\n",
    "    if not symbolic_form.empty:\n",
    "        symbolic_form['reward'] = [final_reward] * symbolic_form.shape[0]\n",
    "        action_steering_info_df.append(action_steering_info)\n",
    "        symbolic_df = pd.concat([symbolic_df, symbolic_form], ignore_index=True)\n",
    "\n",
    "        # Add timestep decisions to graph\n",
    "        timestep_groups = symbolic_form['group'].unique()\n",
    "        for group in timestep_groups:\n",
    "            rt_decision_graph[group].update_graph(symbolic_form[symbolic_form['group'] == group])\n",
    "\n",
    "    symbolizer.step()\n",
    "\n",
    "    grp_str.append(next_obs[0, 14:])\n",
    "    score += final_reward\n",
    "    step_rewards.append(final_reward)\n",
    "    mean_reward = np.mean(step_rewards)\n",
    "    mean_rew.append(mean_reward)\n",
    "\n",
    "    log_print = f'Step: {info[\"current_step\"]} / {env.total_steps - 1} | Agent Action: {ue_select} | Step Reward: {final_reward} | Mean Reward: {mean_reward:.3f} | Score: {score:.3f}'\n",
    "    print(log_print, end=\"\\r\")\n",
    "    # print(\"------------------------------------\")\n",
    "\n",
    "    state_data.append(observation.flatten())\n",
    "    action_reward_data.append([ue_select, reward])\n",
    "    test_buffer.append((observation, action, reward, next_obs, done))\n",
    "    observation = next_obs\n",
    "\n",
    "symbolic_df = symbolic_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS AND PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to show the reward over time with and without action steering\n",
    "action_steering_info_df = pd.DataFrame(action_steering_info_df)\n",
    "action_steering_info_df['timestep'] = action_steering_info_df['timestep'] - action_steering_info_df['timestep'].min() + 1\n",
    "\n",
    "# Create traces for each DataFrame\n",
    "trace1 = go.Scatter(x=action_steering_info_df['timestep'], y=action_steering_info_df['agent_reward'], mode='lines', name='No AS Reward')\n",
    "trace2 = go.Scatter(x=action_steering_info_df['timestep'], y=action_steering_info_df['action_steering_reward'], mode='markers', name='With AS Reward')\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    title='Reward Over Time',\n",
    "    xaxis=dict(title='Timestep'),\n",
    "    yaxis=dict(title='Reward', range=[0, 1]),  # Set y-axis range from 0 to 1\n",
    "    height=1200\n",
    ")\n",
    "\n",
    "# Combine the traces and layout into a figure\n",
    "fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
    "\n",
    "fig.update_layout(font=dict(size=25))\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the difference in reward over time between the agent reward and action steering reward\n",
    "\n",
    "action_steering_info_df['difference'] = action_steering_info_df['action_steering_reward'] - action_steering_info_df['agent_reward']\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=action_steering_info_df['timestep'], \n",
    "    y=action_steering_info_df['difference'], \n",
    "    mode='lines', \n",
    "    name='Reward Difference'\n",
    ")\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    title='Difference of Reward Over Time',\n",
    "    xaxis=dict(title='Timestep'),\n",
    "    yaxis=dict(title='Reward', range=[-1, 1]),  # Set y-axis range from -1 to 1\n",
    "    height=1200\n",
    ")\n",
    "\n",
    "# Combine the traces and layout into a figure\n",
    "fig = go.Figure(data=[trace1], layout=layout)\n",
    "\n",
    "fig.update_layout(font=dict(size=25))\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symbxrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
